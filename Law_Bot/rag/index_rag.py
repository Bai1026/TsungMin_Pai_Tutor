import os
from typing import List, Dict
import re
from pathlib import Path
from dotenv import load_dotenv, find_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ä½¿ç”¨ Gemini ç›¸é—œçš„åŒ¯å…¥
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings

class LawRAGPipeline:
    def __init__(self, google_api_key: str = None, persist_directory: str = None):
        """
        åˆå§‹åŒ–æ³•å¾‹ RAG Pipeline (ä½¿ç”¨ Gemini)
        
        Args:
            google_api_key: Google API é‡‘é‘°
            persist_directory: å‘é‡è³‡æ–™åº«å„²å­˜ç›®éŒ„ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œæœƒæ ¹æ“šæª”æ¡ˆåç¨±è‡ªå‹•ç”¢ç”Ÿï¼‰
        """
        
        # å¦‚æœæ²’æœ‰æä¾› API é‡‘é‘°ï¼Œå˜—è©¦å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥
        if not google_api_key:
            # å˜—è©¦è¼‰å…¥å¤šå€‹å¯èƒ½çš„ .env æª”æ¡ˆä½ç½®
            env_paths = [
                ".env",                                    # ç•¶å‰ç›®éŒ„
                "../.env",                                # ä¸Šå±¤ç›®éŒ„
                "/Users/zoungming/Desktop/Codes/TsungMin_Pai_Tutor/Law_Bot/.env",  # å°ˆæ¡ˆæ ¹ç›®éŒ„
                "/Users/zoungming/Desktop/Codes/TsungMin_Pai_Tutor/Law_Bot/rag/.env"  # rag ç›®éŒ„
            ]
            
            for env_path in env_paths:
                if os.path.exists(env_path):
                    print(f"ğŸ“ è¼‰å…¥ç’°å¢ƒè®Šæ•¸æª”æ¡ˆï¼š{env_path}")
                    load_dotenv(env_path)
                    break
            else:
                print("âš ï¸ æœªæ‰¾åˆ° .env æª”æ¡ˆï¼Œå˜—è©¦ä½¿ç”¨ç³»çµ±ç’°å¢ƒè®Šæ•¸")
            
            # å˜—è©¦å–å¾— API é‡‘é‘°
            google_api_key = (
                os.getenv("GEMINI_API_KEY") or 
                os.getenv("GOOGLE_API_KEY")
            )
            
            if not google_api_key:
                raise ValueError("æœªæ‰¾åˆ° GEMINI_API_KEY æˆ– GOOGLE_API_KEYã€‚è«‹è¨­å®šç’°å¢ƒè®Šæ•¸æˆ–ç›´æ¥å‚³å…¥ api_key åƒæ•¸")
        
        # è¨­å®šç’°å¢ƒè®Šæ•¸
        os.environ["GOOGLE_API_KEY"] = google_api_key
        
        print(f"ğŸ”‘ ä½¿ç”¨ API é‡‘é‘°ï¼š{google_api_key[:10]}...{google_api_key[-5:]}")
        
        try:
            # ä½¿ç”¨ Gemini çš„ embedding æ¨¡å‹
            self.embeddings = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001"  # Gemini çš„ embedding æ¨¡å‹
            )
            print("âœ… Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Embedding æ¨¡å‹åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            raise
        
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.qa_chain = None
        
        # æ³•å¾‹å°ˆç”¨çš„æ–‡æœ¬åˆ†å‰²å™¨è¨­å®š
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]
        )
    
    def parse_law_document(self, text: str) -> List[Document]:
        """
        è§£ææ³•å¾‹è€ƒè©¦æ–‡ä»¶ï¼ŒæŒ‰é¡Œç›®åˆ†å‰²
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            Document åˆ—è¡¨
        """
        documents = []
        
        # æ›´éˆæ´»çš„é¡Œç›®åˆ†å‰²æ¨¡å¼
        patterns = [
            r'\d+\.\s*\d+[å¹´]?[^0-9]*?\d+',  # åŸå§‹æ¨¡å¼
            r'\d+\.\s*[^\n]*',  # ç°¡åŒ–æ¨¡å¼ï¼šæ•¸å­—.é–‹é ­
            r'ã€.*?ã€‘',  # æ¨™é¡Œæ¨¡å¼
            r'ç¬¬\d+é¡Œ',  # é¡Œç›®ç·¨è™Ÿ
        ]
        
        # å…ˆå˜—è©¦åŸå§‹æ¨¡å¼
        matches = []
        for pattern in patterns:
            matches = list(re.finditer(pattern, text))
            if matches:
                print(f"ä½¿ç”¨æ¨¡å¼æ‰¾åˆ° {len(matches)} å€‹åŒ¹é…ï¼š{pattern}")
                break
        
        if not matches:
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šæ¨¡å¼ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            print("æœªæ‰¾åˆ°ç‰¹å®šé¡Œç›®æ¨¡å¼ï¼ŒæŒ‰æ®µè½åˆ†å‰²")
            paragraphs = text.split('\n\n')
            for i, para in enumerate(paragraphs):
                if para.strip() and len(para.strip()) > 50:  # éæ¿¾å¤ªçŸ­çš„æ®µè½
                    doc = Document(
                        page_content=para.strip(),
                        metadata={
                            "title": f"æ®µè½ {i+1}",
                            "section": "æ–‡ä»¶å…§å®¹",
                            "question_number": i + 1,
                            "type": "æ³•å¾‹æ–‡ä»¶"
                        }
                    )
                    documents.append(doc)
        else:
            # æŒ‰æ‰¾åˆ°çš„æ¨¡å¼åˆ†å‰²
            for i, match in enumerate(matches):
                start = match.start()
                end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
                
                question_text = text[start:end].strip()
                
                if question_text:  # ç¢ºä¿å…§å®¹ä¸ç‚ºç©º
                    # æå–é¡Œç›®æ¨™é¡Œ
                    title_match = re.match(r'([^\n]*)', question_text)
                    title = title_match.group(1) if title_match else f"é¡Œç›® {i+1}"
                    
                    # åˆ†æé¡Œç›®çµæ§‹
                    sections = self._extract_sections(question_text)
                    
                    # ç‚ºæ¯å€‹éƒ¨åˆ†å»ºç«‹æ–‡ä»¶
                    for section_name, section_content in sections.items():
                        if section_content.strip():
                            doc = Document(
                                page_content=section_content,
                                metadata={
                                    "title": title,
                                    "section": section_name,
                                    "question_number": i + 1,
                                    "type": "æ³•å¾‹è€ƒå¤é¡Œ"
                                }
                            )
                            documents.append(doc)
        
        print(f"è§£æå‡º {len(documents)} å€‹æ–‡ä»¶ç‰‡æ®µ")
        return documents
    
    def _extract_sections(self, question_text: str) -> Dict[str, str]:
        """
        æå–é¡Œç›®çš„ä¸åŒéƒ¨åˆ†ï¼ˆé¡Œç›®ã€ç­”é¡Œæ¶æ§‹ã€çˆ­é»è¨˜æ†¶ç­‰ï¼‰
        
        Args:
            question_text: é¡Œç›®æ–‡æœ¬
            
        Returns:
            å„éƒ¨åˆ†å…§å®¹çš„å­—å…¸
        """
        sections = {}
        
        # æå–ä¸»è¦æ¡ˆä¾‹äº‹å¯¦
        fact_patterns = [
            r'(ç”².*?è«‹å•.*?ç½ªè²¬ï¼Ÿ)',
            r'(ç”².*?è©¦å•.*?å¦‚ä½•\?)',
            r'(ç”².*?è«‹.*?åˆ†æ)',
            r'(ç”².*?)',  # æ›´å¯¬é¬†çš„åŒ¹é…
        ]
        
        for pattern in fact_patterns:
            fact_match = re.search(pattern, question_text, re.DOTALL)
            if fact_match:
                sections["æ¡ˆä¾‹äº‹å¯¦"] = fact_match.group(1)
                break
        
        # æå–ç­”é¡Œæ¶æ§‹
        structure_pattern = r'ã€ç­”é¡Œæ¶æ§‹ã€‘(.*?)(?:ã€çˆ­é»è¨˜æ†¶ã€‘|$)'
        structure_match = re.search(structure_pattern, question_text, re.DOTALL)
        if structure_match:
            sections["ç­”é¡Œæ¶æ§‹"] = structure_match.group(1)
        
        # æå–çˆ­é»è¨˜æ†¶
        points_pattern = r'ã€çˆ­é»è¨˜æ†¶ã€‘(.*?)(?=\d+\.|$)'
        points_match = re.search(points_pattern, question_text, re.DOTALL)
        if points_match:
            sections["çˆ­é»è¨˜æ†¶"] = points_match.group(1)
        
        # å¦‚æœæ²’æœ‰æ˜ç¢ºçµæ§‹ï¼Œå°±æŠŠæ•´å€‹å…§å®¹ç•¶ä½œä¸€å€‹éƒ¨åˆ†
        if not sections:
            sections["å®Œæ•´å…§å®¹"] = question_text
            
        return sections
    
    def _get_persist_directory(self, file_path: str) -> str:
        """
        æ ¹æ“šæª”æ¡ˆè·¯å¾‘ç”Ÿæˆå°æ‡‰çš„è³‡æ–™åº«ç›®éŒ„è·¯å¾‘
        
        Args:
            file_path: è³‡æ–™æª”æ¡ˆè·¯å¾‘
            
        Returns:
            è³‡æ–™åº«ç›®éŒ„è·¯å¾‘
        """
        # å–å¾—æª”æ¡ˆåç¨±ï¼ˆä¸å«å‰¯æª”åï¼‰
        file_name = Path(file_path).stem
        
        # å»ºç«‹å°æ‡‰çš„è³‡æ–™åº«ç›®éŒ„è·¯å¾‘
        db_path = f"./rag_db/{file_name}_db"
        
        print(f"è‡ªå‹•ç”Ÿæˆè³‡æ–™åº«è·¯å¾‘ï¼š{db_path}")
        return db_path
    
    def index_documents(self, file_path: str):
        """
        ç‚ºæ–‡ä»¶å»ºç«‹å‘é‡ç´¢å¼•
        
        Args:
            file_path: æ–‡ä»¶æª”æ¡ˆè·¯å¾‘
        """
        try:
            # è¨­å®šè³‡æ–™åº«ç›®éŒ„
            self.persist_directory = self._get_persist_directory(file_path)
            
            # è¼‰å…¥æ–‡ä»¶
            print("è¼‰å…¥æ–‡ä»¶...")
            documents = self.parse_law_document_from_file(file_path)
            
            if not documents:
                print("âš ï¸ æœªè§£æå‡ºä»»ä½•æ–‡ä»¶ç‰‡æ®µ")
                return
            
            # åˆ†å‰²æ–‡æœ¬
            print("åˆ†å‰²æ–‡æœ¬...")
            splits = self.text_splitter.split_documents(documents)
            print(f"åˆ†å‰²å¾Œå…± {len(splits)} å€‹ç‰‡æ®µ")
            
            # å»ºç«‹å‘é‡ç´¢å¼•
            print("å»ºç«‹å‘é‡ç´¢å¼•...")
            self.vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            
            # å„²å­˜ç´¢å¼•
            self.vectorstore.persist()
            
            # è¨­å®šå•ç­”éˆ
            self._setup_qa_chain()
            
            print(f"âœ… å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆï¼Œå…± {len(splits)} å€‹ç‰‡æ®µ")
            print(f"ğŸ“ ç´¢å¼•å„²å­˜ä½ç½®ï¼š{self.persist_directory}")
            
        except Exception as e:
            print(f"ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            print("\nå»ºè­°æª¢æŸ¥ï¼š")
            print("1. æ–‡ä»¶è·¯å¾‘æ˜¯å¦æ­£ç¢º")
            print("2. æ–‡ä»¶ç·¨ç¢¼æ˜¯å¦æ­£ç¢º")
            print("3. API é‡‘é‘°æ˜¯å¦æœ‰æ•ˆ")
            print("4. ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸")
            raise
    
    def load_existing_index(self, file_path: str = None):
        """
        è¼‰å…¥ç¾æœ‰çš„å‘é‡ç´¢å¼•
        
        Args:
            file_path: åŸå§‹è³‡æ–™æª”æ¡ˆè·¯å¾‘ï¼ˆç”¨æ–¼ç”Ÿæˆè³‡æ–™åº«è·¯å¾‘ï¼‰
        """
        if file_path:
            self.persist_directory = self._get_persist_directory(file_path)
        
        if not self.persist_directory:
            print("éŒ¯èª¤ï¼šæœªæŒ‡å®šè³‡æ–™åº«è·¯å¾‘ï¼Œè«‹å…ˆå‘¼å« index_documents() æˆ–åœ¨ load_existing_index() ä¸­å‚³å…¥æª”æ¡ˆè·¯å¾‘")
            return
        
        if os.path.exists(self.persist_directory):
            try:
                print(f"è¼‰å…¥ç¾æœ‰ç´¢å¼•ï¼š{self.persist_directory}")
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                self._setup_qa_chain()
                print("âœ… ç¾æœ‰ç´¢å¼•è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ è¼‰å…¥ç¾æœ‰ç´¢å¼•å¤±æ•—ï¼š{e}")
                self.vectorstore = None
        else:
            print(f"ğŸ“ æœªæ‰¾åˆ°ç¾æœ‰ç´¢å¼•ï¼š{self.persist_directory}")
            self.vectorstore = None
    
    def _setup_qa_chain(self):
        """
        è¨­å®šå•ç­”éˆ (ä½¿ç”¨ Gemini)
        """
        # æ³•å¾‹å°ˆç”¨çš„æç¤ºæ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹å­¸è€…ï¼Œå°ˆç²¾æ–¼å°ç£åˆ‘æ³•ã€‚è«‹æ ¹æ“šä»¥ä¸‹ç›¸é—œçš„æ³•å¾‹è³‡æ–™å›ç­”å•é¡Œã€‚

ç›¸é—œè³‡æ–™ï¼š
{context}

å•é¡Œï¼š{question}

è«‹æä¾›è©³ç´°ä¸”æº–ç¢ºçš„æ³•å¾‹åˆ†æï¼ŒåŒ…æ‹¬ï¼š
1. ç›¸é—œæ³•æ¢
2. æ§‹æˆè¦ä»¶åˆ†æ
3. å­¸èªªå’Œå¯¦å‹™è¦‹è§£
4. å…·é«”çš„æ³•å¾‹é©ç”¨

å›ç­”ï¼š"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
        
        # ä½¿ç”¨ Gemini Pro æ¨¡å‹
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=GoogleGenerativeAI(
                model="gemini-pro",  # ä½¿ç”¨ Gemini Pro æ¨¡å‹
                temperature=0.1,
                max_output_tokens=2048  # è¨­å®šæœ€å¤§è¼¸å‡ºé•·åº¦
            ),
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 5}  # Gemini ä¸Šä¸‹æ–‡è¼ƒå¤§ï¼Œå¯ä»¥æª¢ç´¢æ›´å¤šç‰‡æ®µ
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
    
    def query(self, question: str) -> Dict:
        """
        æŸ¥è©¢å•é¡Œ
        
        Args:
            question: æ³•å¾‹å•é¡Œ
            
        Returns:
            åŒ…å«å›ç­”å’Œä¾†æºæ–‡ä»¶çš„å­—å…¸
        """
        if not self.qa_chain:
            raise ValueError("è«‹å…ˆåŸ·è¡Œ index_documents() æˆ– load_existing_index()")
        
        print(f"è™•ç†å•é¡Œï¼š{question}")
        result = self.qa_chain({"query": question})
        
        return {
            "answer": result["result"],
            "source_documents": result["source_documents"]
        }
    
    def search_similar_cases(self, case_description: str, k: int = 3) -> List[Document]:
        """
        æœå°‹ç›¸ä¼¼æ¡ˆä¾‹
        
        Args:
            case_description: æ¡ˆä¾‹æè¿°
            k: è¿”å›çš„ç›¸ä¼¼æ¡ˆä¾‹æ•¸é‡
            
        Returns:
            ç›¸ä¼¼æ¡ˆä¾‹æ–‡ä»¶åˆ—è¡¨
        """
        if not self.vectorstore:
            raise ValueError("è«‹å…ˆåŸ·è¡Œ index_documents() æˆ– load_existing_index()")
        
        return self.vectorstore.similarity_search(case_description, k=k)

# ä½¿ç”¨ç¯„ä¾‹
def main():
    """
    ä¸»è¦åŸ·è¡Œå‡½å¼
    """
    try:
        # åˆå§‹åŒ– RAG pipelineï¼ˆä¸å‚³å…¥ API é‡‘é‘°ï¼Œè®“å®ƒè‡ªå‹•å¾ç’°å¢ƒè®Šæ•¸è¼‰å…¥ï¼‰
        rag = LawRAGPipeline()
        
        # æŒ‡å®šè¦è™•ç†çš„æ–‡ä»¶
        data_file = "/Users/zoungming/Desktop/Codes/TsungMin_Pai_Tutor/Law_Bot/rag/data/specific_offences_ch1.txt"
        
        print(f"ğŸ“ è™•ç†æ–‡ä»¶ï¼š{data_file}")
        
        # å˜—è©¦è¼‰å…¥ç¾æœ‰ç´¢å¼•
        rag.load_existing_index(data_file)
        
        # å¦‚æœæ²’æœ‰ç¾æœ‰ç´¢å¼•ï¼Œå»ºç«‹æ–°çš„
        if not rag.vectorstore:
            print("å»ºç«‹æ–°ç´¢å¼•...")
            rag.index_documents(data_file)
        
        print("ğŸ‰ RAG ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
        
        # ç°¡å–®æ¸¬è©¦
        if rag.qa_chain:
            test_question = "ä»€éº¼æ˜¯ç«Šç›œç½ªï¼Ÿ"
            print(f"\nğŸ§ª æ¸¬è©¦å•é¡Œï¼š{test_question}")
            result = rag.query(test_question)
            print(f"âœ… æ¸¬è©¦æˆåŠŸï¼Œå›ç­”é•·åº¦ï¼š{len(result['answer'])} å­—å…ƒ")
        
    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()